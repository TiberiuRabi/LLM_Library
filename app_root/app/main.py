from typing import List
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI
from app import settings
from rag import retriever
from tools.summaries import get_summary_by_title
from fastapi import HTTPException
import traceback

app = FastAPI(title="Smart Librarian (FastAPI)")
print(settings.OPENAI_API_KEY)
_client = OpenAI(api_key=settings.OPENAI_API_KEY)

try:
    # Option 1: List available models (safe, low-cost)
    models = _client.models.list()
    print("âœ… API Key is valid. Found models like:", [m.id for m in models.data[:3]])
except Exception as e:
    print("âŒ OpenAI API key test failed:")
    print(e)
    raise RuntimeError("ðŸ›‘ Invalid or missing OpenAI API key. Check .env or settings.")
class RecommendRequest(BaseModel):
    query: str
    k: int = 3

class RecommendResponse(BaseModel):
    recommended_title: str
    message: str
    lternatives: List[str]


@app.get("/health")
def health():
    return {"ok": True}

def _ask_llm_to_choose(query: str, candidates: List[dict]) -> dict:
# Ask the LLM to choose the best title and explain briefly (Romanian-friendly)
    titles = [c["title"] for c in candidates]
    sys = (
    "EÈ™ti un bibliotecar prietenos. Ai primit o Ã®ntrebare È™i o listÄƒ de cÄƒrÈ›i candidate. "
    "Alege o singurÄƒ carte care se potriveÈ™te cel mai bine cu tema, apoi rÄƒspunde cu JSON: "
    "{\"title\": str, \"why\": str}. RÄƒspuns scurt (2-4 propoziÈ›ii)."
    )
    usr = {
    "role": "user",
    "content": f"ÃŽntrebare: {query}\nCandidaturi: {titles}"
    }
    chat = _client.chat.completions.create(
        model=settings.OPENAI_MODEL,
        response_format={"type": "json_object"},
        messages=[{"role": "system", "content": sys}, usr],
        temperature=0.5,
    )
    import json

    try:
        data = json.loads(chat.choices[0].message.content)
        if not data.get("title"):
            raise ValueError("Missing title")
        return data
    except Exception as e:
    # Fallback: pick the first candidate
        print(f"Error occurred: {e}")
        return {"title": titles[0], "why": "Se potriveÈ™te cel mai bine dintre rezultatele recuperate."}


@app.post("/recommend", response_model=RecommendResponse)
def recommend(req: RecommendRequest):
    try:
        # 1. Semantic search
        hits = retriever.search(req.query, k=req.k)
        if not hits:
            raise HTTPException(
                status_code=404,
                detail="Nicio potrivire gÄƒsitÄƒ. ÃŽncearcÄƒ o altÄƒ temÄƒ sau adaugÄƒ mai multe cÄƒrÈ›i Ã®n dataset."
            )

        # 2. Ask LLM to choose a match
        choice = _ask_llm_to_choose(req.query, hits)
        title = choice.get("title")
        why = choice.get("why", "")

        if not title:
            raise HTTPException(
                status_code=500,
                detail="RÄƒspunsul de la LLM nu a conÈ›inut un titlu valid."
            )

        # 3. Get full summary
        full = get_summary_by_title(title)
        if not full:
            # Try fallback from metadata
            for h in hits:
                if h["title"].lower() == title.lower():
                    full = h["metadata"].get("full_summary", "")
                    break

        # 4. Format the message
        message = (
            f"ÃŽÈ›i recomand **{title}**. {why}\n\n"
            + (f"**Rezumat complet:** {full}" if full else "(Nu am gÄƒsit rezumatul complet pentru aceastÄƒ carte.)")
        )

        alts = [h["title"] for h in hits if h["title"].lower() != title.lower()]

        return RecommendResponse(
            recommended_title=title,
            message=message,
            alternatives=alts,
        )

    except HTTPException:
        raise  # re-raise known FastAPI errors without wrapping

    except Exception as e:
        traceback.print_exc()  # log full stack trace in terminal
        raise HTTPException(
            status_code=500,
            detail=f"Eroare internÄƒ: {str(e)}"
        )